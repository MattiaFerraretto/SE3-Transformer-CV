wandb:
  api: "-"
  project: "SE3-Transformer"
  run_name: "SE3-unet-v0.0.5.0"

train_set:
  preprocessing: "spatial_transformer"          #choices=['icp', 'spatial_transformer', 'none']
  break_ds_with: "rotation_translation"         #choices=['rotation', 'translation', 'rotation_translation', 'none']
  split: "Train"
  ds_path: "./Facescape"
  category: "Neutral"
  references_pointclouds_icp_path: "./Preprocessing/reference_pointclouds_for_icp"
  reduce_pointcloud_to: 2048

test_set:
  preprocessing: "spatial_transformer"                      #choices=['icp', 'spatial_transformer', 'none']
  break_ds_with: "rotation_translation"                     #choices=['rotation', 'translation', 'rotation_translation', 'none']
  split: "Test"
  ds_path: "./Facescape"
  category: "Neutral"
  references_pointclouds_icp_path: "./Preprocessing/reference_pointclouds_for_icp"
  reduce_pointcloud_to: 2048

model:
  #name: "SE3Unet"
  n_layers: 4
  si_m: "1x1"
  si_e: "att"
  in_features: 3
  hidden_channels: 5  # ref paper 5
  out_features: 68
  pooling_ratio: 0.1
  aggr: "max"

hyper:
  epochs: 25
  learning_rate: 0.0003
  batch_training_size: 1
  batch_eval_size: 1
  device: "cuda"
  gradient_accumulation_steps: 1
  logging_steps: 5
  checkpoint_dir: "checkpoints"
  save_every: 1
  save_max: 6
  optimizer: "Adam"
  optimizer_kwargs: ""
    #momentum: 0.9
  scheduler: "CosineAnelingLr"
  features: "v"
  from_checkpoint: False
  checkpoint_fpath: ""